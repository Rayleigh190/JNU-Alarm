import requests
from bs4 import BeautifulSoup
import pprint
from datetime import datetime

from .models import User, Notification
from .models import Department, Architecture, SoftwareEngineering
from .models import College, Engineering

def send_message(title, body, users, link):
  for user in users:
    Notification.objects.create(user=user, category=title, title=body, link=link)
  print(f"pushed to {users}")
  return

def crawling_job():
  architecture_crawling()
  software_engineering_crawling()
  engineering_crawling()

def general_crawling(base_url, url, department_model):
  response = requests.get(url)
  soup = BeautifulSoup(response.text, 'html.parser')
  posts = []
  last_post = department_model.objects.last()
  for tr in soup.findAll('tr', attrs={'class':''}):
    try:
      if tr.find('td') is None:
        continue
      num = int(tr.find('td', attrs={'class':'td-num'}).text)
      if num <= last_post.num:
        break
      else:
        td = tr.find('td', attrs={'class':'td-subject'})
        title = td.find('strong').text
        href = td.find('a')['href']
        postUrl = base_url + href
        post_data = {
          'num': num,
          'title': title,
          'url': postUrl
        }
        posts.append(post_data)
    except Exception as e:
      print("í¬ë¡¤ë§ì¤‘ ì˜ˆì™¸ ë°œìƒ", e)
      pass
  return posts

## í•™ê³¼ í´ë¡¤ë§
# ê±´ì¶•í•™ë¶€
def architecture_crawling():
  today = str(datetime.now())
  base_url = "https://archi.jnu.ac.kr"
  url = 'https://archi.jnu.ac.kr/archi/8023/subview.do'
  posts = general_crawling(base_url=base_url, url=url, department_model=Architecture)
  
  if len(posts) > 0:
    for post in reversed(posts):
      Architecture.objects.create(num=post['num'], title=post['title'])
      isTrue_departments = Department.objects.filter(architecture=True)
      isTrue_users = User.objects.filter(setting__department__in=isTrue_departments)
      send_message("ê±´ì¶•í•™ë¶€", post['title'], isTrue_users, post['url'])
      print(f"ğŸ  ê±´ì¶•í•™ë¶€ ì•Œë¦¼ ë°œì†¡ : {today} ")
      pprint.pprint(post)
  else:
    print(f"ğŸ  ê±´ì¶•í•™ë¶€ ìƒˆë¡œìš´ ê³µì§€ ì—†ìŒ : {today} ")

# ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™ê³¼
def software_engineering_crawling():
  today = str(datetime.now())
  base_url = "https://sw.jnu.ac.kr"
  url = 'https://sw.jnu.ac.kr/sw/8250/subview.do'
  posts = general_crawling(base_url=base_url, url=url, department_model=SoftwareEngineering)
  
  if len(posts) > 0:
    for post in reversed(posts):
      SoftwareEngineering.objects.create(num=post['num'], title=post['title'])
      # ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™ê³¼ë¥¼ êµ¬ë…í•œ Userì—ê²Œ ì•Œë¦¼ ë°œì†¡
      isTrue_departments = Department.objects.filter(software_engineering=True)
      isTrue_users = User.objects.filter(setting__department__in=isTrue_departments)
      send_message("ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™ê³¼", post['title'], isTrue_users, post['url'])
      print(f"ğŸ’» ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™ê³¼ ì•Œë¦¼ ë°œì†¡ : {today} ")
      pprint.pprint(post)
  else:
    print(f"ğŸ’» ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™ê³¼ ìƒˆë¡œìš´ ê³µì§€ ì—†ìŒ : {today} ")

# ê³µê³¼ëŒ€í•™
def engineering_crawling():
  today = str(datetime.now())
  base_url = "https://eng.jnu.ac.kr"
  url = 'https://eng.jnu.ac.kr/eng/7343/subview.do'
  posts = general_crawling(base_url=base_url, url=url, department_model=Engineering)
  
  if len(posts) > 0:
    for post in reversed(posts):
      Engineering.objects.create(num=post['num'], title=post['title'])
      # ê³µê³¼ëŒ€í•™ì„ êµ¬ë…í•œ Userì—ê²Œ ì•Œë¦¼ ë°œì†¡
      isTrue_college =College.objects.filter(engineering=True)
      isTrue_users = User.objects.filter(setting__college__in=isTrue_college)
      send_message("ê³µê³¼ëŒ€í•™", post['title'], isTrue_users, post['url'])
      print(f"ğŸ› ï¸ ê³µê³¼ëŒ€í•™ ì•Œë¦¼ ë°œì†¡ : {today} ")
      pprint.pprint(post)
  else:
    print(f"ğŸ› ï¸ ê³µê³¼ëŒ€í•™ ìƒˆë¡œìš´ ê³µì§€ ì—†ìŒ : {today} ")